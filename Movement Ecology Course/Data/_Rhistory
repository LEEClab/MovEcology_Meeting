install.packages("bcpa")
library("bcpa", lib.loc="~/R/R-3.4.4/library")
detach("package:bcpa", unload=TRUE)
setwd("C:/MovEcol_RioClaro_2018/Data")
elkDataSrtB <- read.csv("elkDataGpsSrtB.csv")
library(nlme)
library(lattice)
data(Rail)
xyplot(Rail ~ travel, groups = Rail, data=Rail)
fm1Rail.lm <- lm( travel ~ 1, data = Rail )
summary(fm1Rail.lm)
Rail$residsLM1 <- resid(fm1Rail.lm)
plot(residsLM1 ~ factor(Rail), data = Rail)
Rail$residsLM1
View(Rail)
View(Rail)
fm2Rail.lm <- lm( travel ~ Rail - 1, data = Rail )
summary(fm2Rail.lm)
plot(residsLM1 ~ factor(Rail), data = Rail)
fm1Rail.lme <- lme(travel ~ 1, data = Rail, random = ~ 1 | Rail)
summary( fm1Rail.lme )
plot( fm1Rail.lme )
intervals( fm1Rail.lme )
data(ergoStool)
summary(ergoStool)
plot.design( ergoStool )
View(ergoStool)
with(ergoStool, xtabs(effort ~ Type + Subject))
xyplot(effort ~ Type|Subject, data = ergoStool)
fm1Stool <- lme(effort ~ Type, data = ergoStool, random = ~ 1 | Subject)
fm1Stool <- lme(effort ~ Type, data = ergoStool, random = ~ 1 | Subject)
summary(fm1Stool)
fm2Stool <- lm(effort ~ Type + Subject, data = ergoStool)
summary(fm2Stool)
summary(fm1Stool)
summary(fm2Stool)
fm3Stool <- lme(effort ~ 0 + Type, data = ergoStool, random = ~ 1 | Subject)
anova(fm3Stool)
anova(fm1Stool)
with(ergoStool, tapply(effort, Type,mean))
with(ergoStool, tapply(effort, Type,sd))
fm1Stool <- lme(effort ~ Type, data = ergoStool, random = ~ 1 | Subject) ### Es mas efectivo porque no toma en cuenta los datos perdidos y ademas porque el grado de libertad es menor que el anterior
summary(fm1Stool)
fm2Stool <- lm(effort ~ Type + Subject, data = ergoStool)
summary(fm2Stool)
plot(residsLM1 ~ factor(Rail), data = Rail)
fm1Rail.lm <- lm( travel ~ 2, data = Rail )
fm1Rail.lm <- lm( travel ~ 1, data = Rail )
summary(fm1Rail.lm)
Rail$residsLM1 <- resid(fm1Rail.lm)
plot(residsLM1 ~ factor(Rail), data = Rail)
fm2Rail.lm <- lm( travel ~ Rail - 2, data = Rail )
summary(fm2Rail.lm)
fm2Rail.lm <- lm( travel ~ Rail - 1, data = Rail )
summary(fm2Rail.lm)
fm1Rail.lme <- lme(travel ~ 1, data = Rail, random = ~ 2 | Rail)
fm1Rail.lme <- lme(travel ~ 2, data = Rail, random = ~ 1 | Rail)
fm2Rail.lm <- lm( travel ~ Rail, data = Rail )
summary(fm2Rail.lm)
elkDataSrtB <- read.csv("elkDataGpsSrtB.csv")
elkDataSrtB$Date <- as.Date(elkDataSrtB$Date, form = "%Y-%m-%d")
elkDataSrtB$DateTime <- as.POSIXct(elkDataSrtB$timestamp, format="%Y-%m-%d %H:%M:%S", tz = "UTC")
elkDataSrtB$yrDay <- as.numeric(strftime(as.POSIXlt(elkDataSrtB$DateTime),format="%j"))
elkDataSrtB$jday <- round(as.numeric(julian(elkDataSrtB$Date)),0)
elkDataSrtB$serialDay <- elkDataSrtB$jday - min(elkDataSrtB$jday) # i.e. minus the value for first location
elkDataSrtBLS <- split(elkDataSrtB, list(elkDataSrtB$individual.local.identifier))
NSD <- sapply(elkDataSrtBLS, function(x) {
nsd <- (x$utm.easting - x$utm.easting[1])^2 + (x$utm.northing - x$utm.northing[1])^2
})
elkDataSrtBLS
sapply(NSD,summary)
apply(sapply(NSD,summary),2,sqrt)
n <- 1000
rndAnim <- lapply(1:2000, function(i) {
LONGITUDE <- rnorm(n)
LATITUDE <- rnorm(n)
data.frame(LONGITUDE = LONGITUDE, LATITUDE = LATITUDE)
})
str(rndAnim[[1]])
system.time(rnNSD <- sapply(rndAnim, function(x) {
nsd <- (x$LONGITUDE - x$LONGITUDE[1])^2 + (x$LATITUDE - x$LATITUDE[1])^2
})
)
rm(rndAnim, rnNSD)
NSDdf <- data.frame(  IND = as.factor(rep(names(sapply(NSD,function(x) length(x))), as.numeric(sapply(NSD,function(x) length(x))))),
NSD = as.numeric(unlist(NSD))
)
all.equal(elkDataSrtB$individual.local.identifier, NSDdf$IND)
elkDataSrtB$NSD = as.numeric(unlist(NSD))
rm(NSDdf)
nDays <- sapply(elkDataSrtBLS, function(x) {
nday <- (x$jday - min(x$jday))
})
elkDataSrtB$nDays = as.numeric(unlist(nDays))
rm(nDays)
nDays <- sapply(elkDataSrtBLS, function(x) {
nday <- (x$jday - min(x$jday))
})
elkDataSrtB$nDays = as.numeric(unlist(nDays))
rm(nDays)
View(elkDataSrtB)
xyplot(NSD~nDays|individual.local.identifier, data = elkDataSrtB, type=c("l"))
elkDataSrtB$IND <- elkDataSrtB$individual.local.identifier
elkDataSrtC <- aggregate(NSD ~ IND + nDays, data = elkDataSrtB, FUN = mean)
xyplot(NSD~nDays|IND, data = elkDataSrtC, type=c("l"))
elks <- c("YL96","YL92","YL93","YL73","YL74","YL77", "YL78", "YL80", "YL5", "YL58", "YL59", "YL64", "YL15", "YL2", "YL25", "YL29", "YL42", "4049", "GP2", "GR104", "GR182", "GR193")
elkDataSrtD <- subset(elkDataSrtC, elkDataSrtC$IND %in% elks)
elkDataSrtD <- droplevels(elkDataSrtD)
xyplot(NSD~nDays|IND, data = elkDataSrtD, type=c("l"))
elkDataSrtB <- read.csv("elkDataGpsSrtB.csv")
elkDataSrtB$Date <- as.Date(elkDataSrtB$Date, form = "%Y-%m-%d")
elkDataSrtB$DateTime <- as.POSIXct(elkDataSrtB$timestamp, format="%Y-%m-%d %H:%M:%S", tz = "UTC")
elkDataSrtB$yrDay <- as.numeric(strftime(as.POSIXlt(elkDataSrtB$DateTime),format="%j"))
elkDataSrtB$jday <- round(as.numeric(julian(elkDataSrtB$Date)),0)
elkDataSrtB$jday
?round
elkDataSrtB$jday2 <- (as.numeric(julian(elkDataSrtB$Date)),0)
elkDataSrtB$jday2 <- as.numeric(julian(elkDataSrtB$Date))
elkDataSrtB$jday2
all.linearB <- nlme(NSD ~ 4*D*nDays,
data = elkDataSrtD,
fixed = D ~ 1,
random = D ~ 1,
groups = ~ IND,
start = c(D = 1))
library(nlme)
all.linearB <- nlme(NSD ~ 4*D*nDays,
data = elkDataSrtD,
fixed = D ~ 1,
random = D ~ 1,
groups = ~ IND,
start = c(D = 1))
elkDataSrtB <- read.csv("elkDataGpsSrtB.csv")
elkDataSrtB$Date <- as.Date(elkDataSrtB$Date, form = "%Y-%m-%d")
elkDataSrtB$DateTime <- as.POSIXct(elkDataSrtB$timestamp, format="%Y-%m-%d %H:%M:%S", tz = "UTC")
######################################
# add additional time covariates
elkDataSrtB$yrDay <- as.numeric(strftime(as.POSIXlt(elkDataSrtB$DateTime),format="%j"))
elkDataSrtB$jday <- round(as.numeric(julian(elkDataSrtB$Date)),0)
elkDataSrtB$serialDay <- elkDataSrtB$jday - min(elkDataSrtB$jday) # i.e. minus the value for first location
##########################################################################################################################################################################
# transform dataframe to a list, then use sapply(), to speed up the calculations of NSD
# I previously used to calculate NSD for each individual using a for loop -- it works, but can take time with large datasets
# adehabitatLT calculates NSD, but does also a bunch of other stuff and with large datasets I have had memory issues in the past
# and having your own function, for your specific task only, gives you full control
# so, here the function, applied to the elk data, followed by a performance test using a simulated large dataset (2000 animals, with 1000 locations each)
# a good entry point to apply/lapply/sapply ...:
# https://www.r-bloggers.com/apply-lapply-rapply-sapply-functions-in-r/
elkDataSrtBLS <- split(elkDataSrtB, list(elkDataSrtB$individual.local.identifier))
NSD <- sapply(elkDataSrtBLS, function(x) {
nsd <- (x$utm.easting - x$utm.easting[1])^2 + (x$utm.northing - x$utm.northing[1])^2
})
# check the values obtained
sapply(NSD,summary)
apply(sapply(NSD,summary),2,sqrt)
# in the case of Great circle distance, need to load the fossil() library and use this line, instead of the Euclidean distance measure:
# deg.dist(x$LONGITUDE, x$LATITUDE, x$LONGITUDE[1], x$LATITUDE[1]))^2
#####################################################################
# now simulate a huge dataset and test the speed of the NSD function
n <- 1000
rndAnim <- lapply(1:2000, function(i) {
LONGITUDE <- rnorm(n)
LATITUDE <- rnorm(n)
data.frame(LONGITUDE = LONGITUDE, LATITUDE = LATITUDE)
})
str(rndAnim[[1]])
# do not use 'summary()' on this large object
system.time(rnNSD <- sapply(rndAnim, function(x) {
nsd <- (x$LONGITUDE - x$LONGITUDE[1])^2 + (x$LATITUDE - x$LATITUDE[1])^2
})
)
# --> 0.31 seconds only!
################################################################################################
######################
# clean the workspace
rm(rndAnim, rnNSD)
#####################################################################
## now add back NSD values to the dataframe
## given how we have ordered the dataframes and done the operations so far, elkDataSrtB & unlist(NSD) are ordered in the same way
## let us check it
# transform back into a dataframe
NSDdf <- data.frame(  IND = as.factor(rep(names(sapply(NSD,function(x) length(x))), as.numeric(sapply(NSD,function(x) length(x))))),
NSD = as.numeric(unlist(NSD))
)
all.equal(elkDataSrtB$individual.local.identifier, NSDdf$IND)
# --> TRUE
## hence just unlist() the NSD values and add them to the existing elkDataSrtB dataframe
elkDataSrtB$NSD = as.numeric(unlist(NSD))
#######
## NSDdf is not needed, hence we can delete it
rm(NSDdf)
############################################################################
# calculate days from first location for each indivINDual, then add to the DF
nDays <- sapply(elkDataSrtBLS, function(x) {
nday <- (x$jday - min(x$jday))
})
elkDataSrtB$nDays = as.numeric(unlist(nDays))
rm(nDays)
####################################################################################
# plot the NSD patterns
xyplot(NSD~nDays|individual.local.identifier, data = elkDataSrtB, type=c("l"))
####################################################################################
# add a column 'IND' as individual identifier with a shorter name
elkDataSrtB$IND <- elkDataSrtB$individual.local.identifier
####################################################################################
# take out the within-day variability by taking the average NSD per day
elkDataSrtC <- aggregate(NSD ~ IND + nDays, data = elkDataSrtB, FUN = mean)
xyplot(NSD~nDays|IND, data = elkDataSrtC, type=c("l"))
# select only individuals monitored over approx the entire year
elks <- c("YL96","YL92","YL93","YL73","YL74","YL77", "YL78", "YL80", "YL5", "YL58", "YL59", "YL64", "YL15", "YL2", "YL25", "YL29", "YL42", "4049", "GP2", "GR104", "GR182", "GR193")
elkDataSrtD <- subset(elkDataSrtC, elkDataSrtC$IND %in% elks)
elkDataSrtD <- droplevels(elkDataSrtD)
xyplot(NSD~nDays|IND, data = elkDataSrtD, type=c("l"))
#####################################################################################
########################################################################################################################################
# start MSD models - first with daily-scale data
library(nlme)
##############################################################################
## Control parameters in nlme
#  This may not be the best strategy because nlme has iterations within
#  iterations and you are changing the maximum number of iterations for
#  both types.  I'll use Don Watts' terms of "innerations" and
#  "outerations".  The 'maxiter' parameter is the maximum number of
#  "outerations".  Increasing that parameter is fine.  The other parameters
#  control different aspects of the "innerations" which consist of a
#  penalized nonlinear least squares (pnls) step followed by optimization
#  of a locally linear mixed model (llmm).  The llmm step itself has two
#  stages: EM iterations and general optimization.
#
#  The 'niterEM' parameter is the number of iterations that will be done on
#  the first llmm problem.  There is no convergence criterion for the EM
#  iterations.  If you set niterEM = 1000 then it will do exactly 1000
#  iterations.  This will do no harm but it may be a waste of nDays for two
#  reasons - EM iterations are slow to converge and these iterations apply
#  wrong problem.  The default is 25.  I somenDayss increase it to 100 but
#  rarely beyond that.
#
#  the general optimization stage of each llmm step.  That could be
#  increased but probably not beyond a couple of hundred.  If the optimizer
#  can't converge after, say, 500 iterations then the llmm step is in trouble.
#  The 'pnlsMaxIter' parameter is the maximum number of iterations in each
#  pnls step.  The default is 7.  It is not a good idea to make this very
#  large because getting an accurate answer to a pnls step, especially the
#  to the wrong problem.  The idea is that you only take a few steps in the
#  early pnls problems, then switch to the llmm problem which will provide
#  a different pnls problem to solve.
# steps will terminate earlier.  It is unlikely that decreasing this
# The overall convergence is declared when the relative change in the
# parameters after a pnls/llmm pair is below the convergence criterion.
# The llmm problem is always
# parameterized in terms of the relative precision matrices, which are the
# inverses of the relative variance-covariance matrices.  A variance of
# finite parameter values.
##############################################################################
library(nlme)
#-> D = diffusion coef > 4*D*hours gives a parameter of space use
#####################################################################################################
all.linearB <- nlme(NSD ~ 4*D*nDays,
data = elkDataSrtD,
# zero corresponds to an infinite precision and cannot be represented by
# parameter will help ensure convergence.
#
#  first few, is another case of worrying about getting an accurate answer
#  The 'msMaxIter' parameter is the maximum number of iterations allowed in
#### nomad
# if you increase minScale then the pnls
#  to the first llmm problem, which may be changed radically by the next
#  pnls step so you are spending nDays getting an accurate answer to the
# The pnlsTol applies only to the pnls step.
all.linearB <- nlme(NSD ~ 4*D*nDays,
data = elkDataSrtD,
fixed = D ~ 1,
random = D ~ 1,
groups = ~ IND,
start = c(D = 1))
all.linearB <- nlme(NSD ~ 4*D*nDays,
data = elkDataSrtD,
fixed = D ~ 1,
random = D ~ 1,
groups = ~ IND,
start = c(D = 1))
D = diffusion coef > 4*D*hours
start = c(D = 100))
start = c(D = 100))
verbose=True)
start = c(D = 100))
all.linearB <- nlme(NSD ~ 4*D*nDays,data = elkDataSrtD, fixed = D ~ random = D ~ 1, groups = ~ IND, start = c(D = 100))
all.linearB <- nlme(NSD ~ 4*D*nDays,data = elkDataSrtD, fixed = D ~ random = D ~ 1, groups = ~ IND, start = c(D = 100))
all.linearB <- nlme(NSD ~ 4*D*nDays, data = elkDataSrtD, fixed = D ~ random = D ~ 1, groups = ~ IND, start = c(D = 100))
ranef1.Dispmod <- nlme(NSD ~ Asym/(1 + exp((xmid-nDays)/scal)),
data = elkDataSrtD,
fixed = Asym + xmid + scal ~ 1,
random = Asym ~ 1,
groups = ~ IND,
na.action = na.exclude,
start = c(Asym = summary(null.HRmod)$tTable[1], xmid = 1, scal = 1),verbose=F)
ranef1.Dispmod <- nlme(NSD ~ Asym/(1 + exp((xmid-nDays)/scal)),
data = elkDataSrtD,
fixed = Asym + xmid + scal ~ 1, ###name of parameter function/flexion point
random = Asym ~ 1,
groups = ~ IND,
na.action = na.exclude,
start = c(Asym = summary(null.HRmod)$tTable[1], xmid = 1, scal = 1),verbose=F)
full.Dispmod <- nlme(NSD ~ Asym/(1 + exp((xmid-nDays)/scal)),
data = elkDataSrtD,
fixed = Asym + xmid + scal ~ 1,
random = Asym + xmid + scal ~ 1,##
groups = ~ IND,
na.action = na.exclude,
control = nlmeControl(pnlsMaxIter=20, niterEM = 25, returnObject=TRUE),
start = c(Asym = summary(ranef1.Dispmod)$tTable[1], xmid = summary(ranef1.Dispmod)$tTable[2], scal = summary(ranef1.Dispmod)$tTable[3]),
verbose = T)
